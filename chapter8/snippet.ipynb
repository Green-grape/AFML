{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b43ef91",
   "metadata": {},
   "source": [
    "AFML chapter 8 snippet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c669b5d",
   "metadata": {},
   "source": [
    "8.2 MDI(Mean Decrease Impurity) Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19256bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "def get_mdi_importance(fit, feature_names):\n",
    "    df0={i:tree.feature_importances_ for i,tree in enumerate(fit.estimators_)}\n",
    "    df0=pd.DataFrame.from_dict(df0, orient='index')\n",
    "    df0.columns=feature_names\n",
    "    df0=df0.replace(0, np.nan) # zero importance가 average되는 것을 방지\n",
    "    imp=pd.concat({'mean':df0.mean(axis=0).rename('mean'), 'std':df0.std(axis=0).rename('std')/np.sqrt(len(df0))}, axis=1)\n",
    "    imp/=imp['mean'].sum() # normalize  \n",
    "    return imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd34efe",
   "metadata": {},
   "source": [
    "8.3 MDA(Mean Decrease Accuracy) Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00e03102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import PurgedKFold\n",
    "import time\n",
    "\n",
    "def get_mda_importance(clf, X, y, cv, sample_weight:pd.Series, t1:pd.Series, pct_embargo:float, scoring='neg_log_loss'):\n",
    "    # get feature importance based on OOS score reduction\n",
    "    if scoring not in ['neg_log_loss', 'accuracy']:\n",
    "        raise ValueError(f\"scoring must be 'neg_log_loss' or 'accuracy', got {scoring}\")\n",
    "    from sklearn.metrics import log_loss, accuracy_score\n",
    "    cv_gen=PurgedKFold(n_splits=cv, t1=t1, pct_embargo=pct_embargo)\n",
    "    scr0, scr1=pd.Series(), pd.DataFrame(columns=X.columns)\n",
    "    for i, (train_idx, test_idx) in enumerate(cv_gen.split(X, y)):\n",
    "        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "        X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n",
    "        sample_weight_train=sample_weight.iloc[train_idx]\n",
    "        sample_weight_test=sample_weight.iloc[test_idx]\n",
    "        clf.fit(X_train, y_train, sample_weight=sample_weight_train)\n",
    "        if scoring=='neg_log_loss':\n",
    "            scr0.loc[i]=-log_loss(y_test, clf.predict_proba(X_test), sample_weight=sample_weight_test.values, labels=clf.classes_)\n",
    "        else:\n",
    "            scr0.loc[i]=accuracy_score(y_test, clf.predict(X_test), sample_weight=sample_weight_test.values)\n",
    "        for j in X.columns:\n",
    "            # inplace 전체 copy 제거 + numpy array shuffle\n",
    "            shuffled_col = X_test[j].values.copy()\n",
    "            np.random.shuffle(shuffled_col)\n",
    "            X1_ = X_test.copy()\n",
    "            X1_[j] = shuffled_col\n",
    "            if scoring=='neg_log_loss':\n",
    "                scr1.loc[i,j]=-log_loss(y_test, clf.predict_proba(X1_), sample_weight=sample_weight_test.values, labels=clf.classes_)\n",
    "            else:\n",
    "                scr1.loc[i,j]=accuracy_score(y_test, clf.predict(X1_), sample_weight=sample_weight_test.values)\n",
    "    imp=(-scr1).add(scr0, axis=0)\n",
    "    if scoring=='neg_log_loss':\n",
    "        imp=imp/-scr1 # (original - permuted) / permuted\n",
    "    else:\n",
    "        imp=imp/(1.0-scr1)\n",
    "    imp=pd.concat({'mean':imp.mean(axis=0).rename('mean'), 'std':imp.std(axis=0).rename('std')/np.sqrt(len(imp))}, axis=1)\n",
    "    return imp, scr0.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b86e44f",
   "metadata": {},
   "source": [
    "8.4 SFI(Single Feature Importance) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "863ba1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import cv_score\n",
    "\n",
    "def get_sfi(feature_names, clf, normalizd_X, y:pd.Series,sample_weight:pd.Series, scoring, cv_gen)->pd.DataFrame:\n",
    "    imp=pd.DataFrame(columns=['mean', 'std'])\n",
    "    for feature in feature_names:\n",
    "        df0=cv_score(clf, X=normalizd_X[[feature]], y=y, sample_weight=sample_weight, scoring=scoring, cv_gen=cv_gen)\n",
    "        imp.loc[feature, 'mean']=df0.mean(axis=0)\n",
    "        imp.loc[feature, 'std']=df0.std(axis=0)/np.sqrt(len(df0))\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be84954",
   "metadata": {},
   "source": [
    "8.5 Computation of Orthogonal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8588cdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orthogonal_features(X:pd.DataFrame, threshold:float=0.95)->pd.DataFrame:\n",
    "    def get_eigen(dot:pd.DataFrame, threshold:float=0.95):\n",
    "        eigen_values, eigen_vectors = np.linalg.eig(dot)\n",
    "\n",
    "        # reverse\n",
    "        idx=eigen_values.argsort()[::-1] \n",
    "        eigen_values=eigen_values[idx]\n",
    "        eigen_vectors=eigen_vectors[:,idx]\n",
    "\n",
    "        eigen_values=pd.Series(eigen_values, index=['PC_'+str(i+1) for i in range(len(eigen_values))])\n",
    "        eigen_vectors=pd.DataFrame(eigen_vectors, index=dot.index, columns=eigen_values.index)\n",
    "        eigen_vectors=eigen_vectors.loc[: eigen_values.index]\n",
    "\n",
    "        cum_var=eigen_values.cumsum()/eigen_values.sum()\n",
    "        dim=cum_var.values.searchsorted(threshold)\n",
    "        eigen_values, eigen_vectors=eigen_values.iloc[:dim+1], eigen_vectors.iloc[:,:dim+1]\n",
    "        return eigen_values, eigen_vectors\n",
    "    Z=X.subtract(X.mean(), axis=1).div(X.std(), axis=1) \n",
    "    dot=pd.DataFrame(np.dot(Z.T, Z), index=X.columns, columns=X.columns)\n",
    "    eigen_values, eigen_vectors=get_eigen(dot, threshold)\n",
    "    return np.dot(Z, eigen_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29767a5",
   "metadata": {},
   "source": [
    "8.7 Creating A Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4cb7c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 I_0       I_1       I_2       I_3       I_4  \\\n",
      "1997-12-26 21:22:15.218328  2.843740  0.456554  0.171107 -4.511382  0.278990   \n",
      "1997-12-27 21:22:15.218328  3.561541 -1.566097  3.342813 -1.938909  2.075749   \n",
      "1997-12-28 21:22:15.218328  7.699248 -3.030124 -0.859302 -0.033351  1.113719   \n",
      "1997-12-29 21:22:15.218328 -0.149801 -3.182187  2.695894  1.359997  2.992416   \n",
      "1997-12-30 21:22:15.218328 -2.157903  0.046380  0.697217 -1.012036  1.856002   \n",
      "\n",
      "                                 I_5       I_6       I_7       I_8       I_9  \\\n",
      "1997-12-26 21:22:15.218328 -3.474726  2.955550  2.698865  1.542440  2.198168   \n",
      "1997-12-27 21:22:15.218328 -3.486711  0.494908  0.309615  1.059439 -0.792433   \n",
      "1997-12-28 21:22:15.218328 -0.877844  2.344033  4.089113  2.287786  0.611413   \n",
      "1997-12-29 21:22:15.218328 -0.417971 -1.214058  1.268313 -3.720913 -2.580578   \n",
      "1997-12-30 21:22:15.218328 -2.311465  2.715493  0.444433 -1.921790 -2.472372   \n",
      "\n",
      "                            ...      N_30      N_31      N_32      N_33  \\\n",
      "1997-12-26 21:22:15.218328  ... -0.330515 -0.845502 -1.477466  1.217536   \n",
      "1997-12-27 21:22:15.218328  ... -0.020384 -0.751467  0.212077  0.285038   \n",
      "1997-12-28 21:22:15.218328  ...  0.744056  0.914181  1.586483  0.692802   \n",
      "1997-12-29 21:22:15.218328  ... -1.960632 -2.064914  1.258648 -1.031856   \n",
      "1997-12-30 21:22:15.218328  ... -0.841121  0.081347 -2.587682 -0.416436   \n",
      "\n",
      "                                N_34      N_35      N_36      N_37      N_38  \\\n",
      "1997-12-26 21:22:15.218328  0.304644  1.557365  0.202843  0.160110  0.933805   \n",
      "1997-12-27 21:22:15.218328  0.125461  0.203534 -0.376495 -0.938780 -0.142879   \n",
      "1997-12-28 21:22:15.218328 -0.953431  0.679360  0.565153  0.219302 -1.110504   \n",
      "1997-12-29 21:22:15.218328  0.645146 -0.063900  0.305844  0.371489  3.218969   \n",
      "1997-12-30 21:22:15.218328 -1.077859 -0.428086 -0.183735 -0.434254 -2.124955   \n",
      "\n",
      "                                N_39  \n",
      "1997-12-26 21:22:15.218328 -0.132272  \n",
      "1997-12-27 21:22:15.218328  0.533263  \n",
      "1997-12-28 21:22:15.218328 -1.086061  \n",
      "1997-12-29 21:22:15.218328  0.867178  \n",
      "1997-12-30 21:22:15.218328 -0.709056  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "                            bin       w                         t1\n",
      "1997-12-26 21:22:15.218328    0  0.0001 1997-12-26 21:22:15.218328\n",
      "1997-12-27 21:22:15.218328    0  0.0001 1997-12-27 21:22:15.218328\n",
      "1997-12-28 21:22:15.218328    0  0.0001 1997-12-28 21:22:15.218328\n",
      "1997-12-29 21:22:15.218328    0  0.0001 1997-12-29 21:22:15.218328\n",
      "1997-12-30 21:22:15.218328    0  0.0001 1997-12-30 21:22:15.218328\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_test_data(n_features=40, n_informative=10, n_redundant=10, n_samples=10000):\n",
    "    from sklearn.datasets import make_classification\n",
    "    X, cont=make_classification(n_samples=n_samples, n_features=n_features, n_informative=n_informative, n_redundant=n_redundant, random_state=0, shuffle=False)\n",
    "    df0=pd.date_range(periods=n_samples, freq='1D', end=datetime.today())\n",
    "    X, cont=pd.DataFrame(X, index=df0), pd.Series(cont, index=df0).to_frame('bin')\n",
    "    df0=[f'I_{str(i)}' for i in range(n_informative)]+[f'R_{str(i)}' for i in range(n_informative, n_informative+n_redundant)]+[f'N_{str(i)}' for i in range(n_informative+n_redundant, n_features)]\n",
    "    X.columns=df0\n",
    "    cont['w']=1./cont.shape[0]\n",
    "    cont['t1']=pd.Series(cont.index, index=cont.index)\n",
    "    return X, cont\n",
    "\n",
    "test_data, cont=get_test_data()\n",
    "print(test_data.head())\n",
    "print(cont.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83597102",
   "metadata": {},
   "source": [
    "8.8 Calling Feature Importance for any method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3b534ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(trans_x, cont, n_estimators=1000, cv=10, max_samples=1, num_threads=24, pct_embargo=0.1, scoring='accuracy', method='SFI', min_weight_leaf=0, **kwargs):\n",
    "    '''\n",
    "    Random Forest Feature Importance\n",
    "    '''\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "    clf=DecisionTreeClassifier(criterion='entropy',max_features=1, class_weight='balanced', min_weight_fraction_leaf=min_weight_leaf)\n",
    "    clf=BaggingClassifier(estimator=clf, n_estimators=n_estimators, max_features=1., max_samples=max_samples, oob_score=True)\n",
    "    fit=clf.fit(X=trans_x, y=cont['bin'], sample_weight=cont['w'].values)\n",
    "    oob_score=fit.oob_score_\n",
    "    if method=='MDI':\n",
    "        imp=get_mdi_importance(fit, feature_names=trans_x.columns)\n",
    "        oos=cv_score(clf, X=trans_x, y=cont['bin'], sample_weight=cont['w'], scoring=scoring, cv_gen=PurgedKFold(n_splits=cv, t1=cont['t1'], pct_embargo=pct_embargo)).mean()\n",
    "    elif method=='MDA':\n",
    "        imp, oos=get_mda_importance(clf, X=trans_x, y=cont['bin'], cv=cv, sample_weight=cont['w'], t1=cont['t1'], pct_embargo=pct_embargo, scoring=scoring)\n",
    "    elif method=='SFI':\n",
    "        cv_gen=PurgedKFold(n_splits=cv, t1=cont['t1'], pct_embargo=pct_embargo)\n",
    "        oos=cv_score(clf, X=trans_x, y=cont['bin'], sample_weight=cont['w'], scoring=scoring, cv_gen=cv_gen).mean()\n",
    "        imp=get_sfi(trans_x.columns, clf, trans_x, y=cont['bin'], sample_weight=cont['w'], scoring=scoring, cv_gen=cv_gen)\n",
    "    return imp, oob_score, oos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663537fc",
   "metadata": {},
   "source": [
    "8.9 Calling All components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df23dbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimjuho/Desktop/quant/AFML/chapter8/utils.py:72: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_t1_idx = self.t1.index.searchsorted(self.t1[test_indices].max())\n",
      "/Users/kimjuho/Desktop/quant/AFML/chapter8/utils.py:72: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_t1_idx = self.t1.index.searchsorted(self.t1[test_indices].max())\n",
      "/Users/kimjuho/Desktop/quant/AFML/chapter8/utils.py:72: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_t1_idx = self.t1.index.searchsorted(self.t1[test_indices].max())\n",
      "/Users/kimjuho/Desktop/quant/AFML/chapter8/utils.py:72: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_t1_idx = self.t1.index.searchsorted(self.t1[test_indices].max())\n",
      "/Users/kimjuho/Desktop/quant/AFML/chapter8/utils.py:72: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_t1_idx = self.t1.index.searchsorted(self.t1[test_indices].max())\n",
      "/Users/kimjuho/Desktop/quant/AFML/chapter8/utils.py:72: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_t1_idx = self.t1.index.searchsorted(self.t1[test_indices].max())\n",
      "/Users/kimjuho/Desktop/quant/AFML/chapter8/utils.py:72: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_t1_idx = self.t1.index.searchsorted(self.t1[test_indices].max())\n",
      "/Users/kimjuho/Desktop/quant/AFML/chapter8/utils.py:72: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_t1_idx = self.t1.index.searchsorted(self.t1[test_indices].max())\n",
      "/Users/kimjuho/Desktop/quant/AFML/chapter8/utils.py:72: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_t1_idx = self.t1.index.searchsorted(self.t1[test_indices].max())\n",
      "/Users/kimjuho/Desktop/quant/AFML/chapter8/utils.py:72: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_t1_idx = self.t1.index.searchsorted(self.t1[test_indices].max())\n",
      "/Users/kimjuho/Desktop/quant/AFML/chapter8/utils.py:72: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_t1_idx = self.t1.index.searchsorted(self.t1[test_indices].max())\n",
      "/Users/kimjuho/Desktop/quant/AFML/chapter8/utils.py:72: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_t1_idx = self.t1.index.searchsorted(self.t1[test_indices].max())\n",
      "/Users/kimjuho/Desktop/quant/AFML/chapter8/utils.py:72: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_t1_idx = self.t1.index.searchsorted(self.t1[test_indices].max())\n",
      "/Users/kimjuho/Desktop/quant/AFML/chapter8/utils.py:72: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_t1_idx = self.t1.index.searchsorted(self.t1[test_indices].max())\n",
      "/Users/kimjuho/Desktop/quant/AFML/chapter8/utils.py:72: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_t1_idx = self.t1.index.searchsorted(self.t1[test_indices].max())\n",
      "/Users/kimjuho/Desktop/quant/AFML/chapter8/utils.py:72: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_t1_idx = self.t1.index.searchsorted(self.t1[test_indices].max())\n",
      "/Users/kimjuho/Desktop/quant/AFML/chapter8/utils.py:72: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_t1_idx = self.t1.index.searchsorted(self.t1[test_indices].max())\n",
      "/Users/kimjuho/Desktop/quant/AFML/chapter8/utils.py:72: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_t1_idx = self.t1.index.searchsorted(self.t1[test_indices].max())\n",
      "/Users/kimjuho/Desktop/quant/AFML/chapter8/utils.py:72: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_t1_idx = self.t1.index.searchsorted(self.t1[test_indices].max())\n",
      "/Users/kimjuho/Desktop/quant/AFML/chapter8/utils.py:72: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_t1_idx = self.t1.index.searchsorted(self.t1[test_indices].max())\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_sfi() got an unexpected keyword argument 'sample_weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     36\u001b[39m     out=out[\u001b[33m'\u001b[39m\u001b[33mmethod\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mscoring\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmax_samples\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mI\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mR\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mN\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33moob\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33moos\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     37\u001b[39m     out.to_csv(\u001b[33m'\u001b[39m\u001b[33mfeature_importance.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mtest_feature_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_informative\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_redundant\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mtest_feature_importance\u001b[39m\u001b[34m(n_features, n_informative, n_redundant, n_estimators, n_samples, cv)\u001b[39m\n\u001b[32m     24\u001b[39m out=[]\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m job \u001b[38;5;129;01min\u001b[39;00m jobs:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     imp, oob, oos=\u001b[43mget_feature_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrans_x\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrans_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcont\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcont\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     kargs.update(job)\n\u001b[32m     28\u001b[39m     plot_feature_importance(imp, oob, oos, **kargs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mget_feature_importance\u001b[39m\u001b[34m(trans_x, cont, n_estimators, cv, max_samples, num_threads, pct_embargo, scoring, method, min_weight_leaf, **kwargs)\u001b[39m\n\u001b[32m     18\u001b[39m     cv_gen=PurgedKFold(n_splits=cv, t1=cont[\u001b[33m'\u001b[39m\u001b[33mt1\u001b[39m\u001b[33m'\u001b[39m], pct_embargo=pct_embargo)\n\u001b[32m     19\u001b[39m     oos=cv_score(clf, X=trans_x, y=cont[\u001b[33m'\u001b[39m\u001b[33mbin\u001b[39m\u001b[33m'\u001b[39m], sample_weight=cont[\u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m], scoring=scoring, cv_gen=cv_gen).mean()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     imp=\u001b[43mget_sfi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrans_x\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcont\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbin\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcont\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv_gen\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv_gen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m imp, oob_score, oos\n",
      "\u001b[31mTypeError\u001b[39m: get_sfi() got an unexpected keyword argument 'sample_weight'"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "def test_feature_importance(n_features=40, n_informative=10, n_redundant=10, n_estimators=1000, n_samples=10000, cv=10):\n",
    "    trans_x, cont=get_test_data(n_features=n_features, n_informative=n_informative, n_redundant=n_redundant, n_samples=n_samples)\n",
    "    dict0={'min_weight_leaf':[0.], 'scoring':['accuracy'], 'method':['MDA', 'SFI'], 'max_samples':[1.]}\n",
    "    jobs, out=(dict(zip(dict0.keys(), values)) for values in product(*dict0.values())), []\n",
    "    kargs={'n_estimators':n_estimators, 'tag':'test_func', 'cv': cv}\n",
    "\n",
    "    def plot_feature_importance(imp, oob, oos, method, scoring, **kargs):\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(10, imp.shape[0]/5))\n",
    "        imp=imp.sort_values('mean', ascending=True)\n",
    "        ax=imp['mean'].plot(xerr=imp['std'], error_kw={'ecolor':'r'}, color='b', kind='barh')    \n",
    "        if method=='MDI':\n",
    "            plt.xlim([0, imp.sum(axis=1).max()])\n",
    "            plt.axvline(1/len(imp), color='r', linestyle='--')\n",
    "        for i, j in zip(ax.patches, imp.index):\n",
    "            ax.text(i.get_width()/2, i.get_y()+i.get_height()/2, j, ha='center', va='center', fontsize=10)\n",
    "        plt.title(f\"{method} Feature Importance\\nOOB: {oob:.4f}, OOS: {oos:.4f}\", fontsize=15)\n",
    "        plt.savefig(f\"feature_importance_{method}_{scoring}.png\", dpi=300)\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        \n",
    "    out=[]\n",
    "    for job in jobs:\n",
    "        imp, oob, oos=get_feature_importance(trans_x=trans_x,cont=cont, **job)\n",
    "        kargs.update(job)\n",
    "        plot_feature_importance(imp, oob, oos, **kargs)\n",
    "        df0=imp[['mean']]/imp[['mean']].abs().sum() \n",
    "        df0['type']=[i[0] for i in df0.index]   \n",
    "        df0=df0.groupby('type').sum().to_dict()\n",
    "        df0.update({'oob':oob, 'oos':oos})\n",
    "        df0.update(job)\n",
    "        out.append(df0)\n",
    "    out=pd.DataFrame(out).sort_values(['method', 'scoring', 'max_samples', 'min_weight_leaf'])  \n",
    "    out=out['method', 'scoring', 'max_samples', 'I', 'R', 'N', 'oob', 'oos']\n",
    "    out.to_csv('feature_importance.csv')\n",
    "\n",
    "\n",
    "test_feature_importance(n_features=40, n_informative=10, n_redundant=10, n_estimators=1000, n_samples=10000, cv=10)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b32b9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AFML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
